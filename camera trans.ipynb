{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d2e7577-5644-4333-8dd9-67c37a8a4529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL+H model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL+H model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL+H model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL+H model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL+H model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL+H model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/x_hensh/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['requests>=2.32.2', 'tqdm>=4.66.3', 'setuptools>=70.0.0'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 30] Read-only file system: '/software/sse/manual/PyTorch/2.3.0/python-3.10/envs/pytorch_2.3.0/lib/python3.10/site-packages/tqdm'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"requests>=2.32.2\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" ' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 30] Read-only file system: '/software/sse/manual/PyTorch/2.3.0/python-3.10/envs/pytorch_2.3.0/lib/python3.10/site-packages/tqdm'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"requests>=2.32.2\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" ' returned non-zero exit status 1.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m âŒ Command 'pip install --no-cache-dir \"requests>=2.32.2\" \"tqdm>=4.66.3\" \"setuptools>=70.0.0\" ' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2024-12-10 Python-3.10.0 torch-1.13.1+cu117 CUDA:0 (NVIDIA A100-SXM4-80GB, 81051MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "sys.path.insert(0, \"main\")\n",
    "sys.path.insert(0, \"./\")\n",
    "\n",
    "from common.utils.preprocessing import load_img, process_bbox, generate_patch_image\n",
    "from config import cfg\n",
    "\n",
    "\n",
    "\n",
    "img_path = 'demo/input.png'\n",
    "original_img = load_img(img_path)\n",
    "original_img_height, original_img_width = original_img.shape[:2]\n",
    "\n",
    "detector = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "with torch.no_grad():\n",
    "    results = detector(original_img)\n",
    "    \n",
    "person_results = results.xyxy[0][results.xyxy[0][:, 5] == 0]\n",
    "class_ids, confidences, boxes = [], [], []\n",
    "for detection in person_results:\n",
    "    x1, y1, x2, y2, confidence, class_id = detection.tolist()\n",
    "    class_ids.append(class_id)\n",
    "    confidences.append(confidence)\n",
    "    boxes.append([x1, y1, x2 - x1, y2 - y1])\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "persons_deteted = []\n",
    "for num, indice in enumerate(indices):\n",
    "    bbox = boxes[indice]  # x,y,h,w\n",
    "    bbox = process_bbox(bbox, original_img_width, original_img_height)\n",
    "    img, img2bb_trans, bb2img_trans = generate_patch_image(original_img, bbox, 1.0, 0.0, False, cfg.input_img_shape)\n",
    "    persons_deteted.append(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2857c120-57d1-4a7a-a742-357d2fe793f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x_hensh/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "\u001b[92m12-15 23:24:17\u001b[0m Load checkpoint from pretrained_models/osx_l.pth.tar\n",
      "\u001b[92m12-15 23:24:17\u001b[0m Creating graph...\n",
      "\u001b[92m12-15 23:24:28\u001b[0m Load checkpoint from pretrained_models/osx_l.pth.tar\n",
      "\u001b[92m12-15 23:24:28\u001b[0m Load checkpoint from pretrained_models/osx_l.pth.tar\n",
      "\u001b[92m12-15 23:24:28\u001b[0m Creating graph...\n",
      "\u001b[92m12-15 23:24:28\u001b[0m Creating graph...\n",
      "\u001b[92m12-15 23:24:35\u001b[0m Load checkpoint from pretrained_models/osx_l.pth.tar\n",
      "\u001b[92m12-15 23:24:35\u001b[0m Load checkpoint from pretrained_models/osx_l.pth.tar\n",
      "\u001b[92m12-15 23:24:35\u001b[0m Load checkpoint from pretrained_models/osx_l.pth.tar\n",
      "\u001b[92m12-15 23:24:35\u001b[0m Creating graph...\n",
      "\u001b[92m12-15 23:24:35\u001b[0m Creating graph...\n",
      "\u001b[92m12-15 23:24:35\u001b[0m Creating graph...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.path.insert(0, \"main/transformer_utils\")\n",
    "\n",
    "from common.base import Demoer\n",
    "from common.utils.human_models import smpl_x\n",
    "from common.utils.human_models import smpl_h\n",
    "from common.utils.human_models import smil_h\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "cfg.set_additional_args(\n",
    "    encoder_setting='osx_l',\n",
    "    decoder_setting='wo_face_decoder', \n",
    "    pretrained_model_path='pretrained_models/osx_l.pth.tar')\n",
    "\n",
    "demoerx = Demoer()\n",
    "demoerx._make_model(smpl_x)\n",
    "demoerx.model.eval()\n",
    "\n",
    "demoerh = Demoer()\n",
    "demoerh._make_model(smpl_h)\n",
    "demoerh.model.eval()\n",
    "\n",
    "demoeri = Demoer()\n",
    "demoeri._make_model(smil_h)\n",
    "demoeri.model.eval()\n",
    "\n",
    "outx = []\n",
    "outh = []\n",
    "outi = []\n",
    "\n",
    "for person_detected in persons_deteted:\n",
    "    img = transform(person_detected.astype(np.float32))/255\n",
    "    img = img.cuda()[None,:,:,:]\n",
    "    inputs = {'img': img}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outx.append(demoerx.model(inputs, {}, {}, 'test'))\n",
    "        outh.append(demoerh.model(inputs, {}, {}, 'test'))\n",
    "        outi.append(demoeri.model(inputs, {}, {}, 'test'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bb9e2e-e399-40c8-bb61-d010ffdc2243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'joint_img', 'smplx_joint_proj', 'smplx_mesh_cam', 'smplx_root_pose', 'smplx_body_pose', 'smplx_lhand_pose', 'smplx_rhand_pose', 'smplx_jaw_pose', 'smplx_shape', 'smplx_expr', 'cam_trans', 'lhand_bbox', 'rhand_bbox', 'face_bbox', 'outpu'])\n",
      "dict_keys(['vertices', 'joints', 'full_pose', 'global_orient', 'transl', 'betas', 'body_pose', 'left_hand_pose', 'right_hand_pose', 'expression', 'jaw_pose'])\n"
     ]
    }
   ],
   "source": [
    "print (outx[0].keys())\n",
    "print (outx[0]['outpu'].__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce9c0c89-1cdb-4e2d-a1df-268408616d0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.44423175 0.33620498 -0.9853153 0.60636604 -0.5034078 0.34681487\n",
      "-0.25971022 0.23678397 -1.102833 0.5907675 -0.25650448 0.18536916\n",
      "-0.44815582 0.32510862 -1.0463173 0.53509444 -0.47288802 0.19826841\n",
      "-0.27214545 0.2810422 -1.1034721 0.58160144 -0.26713383 0.18028995\n",
      "-0.29607275 0.34982184 -1.1026102 0.5958487 -0.28598312 0.17209314\n",
      "-0.39707083 0.26880464 -1.0667359 0.4525643 -0.61781174 0.22608219\n"
     ]
    }
   ],
   "source": [
    "for item in outx:\n",
    "    print (item['outpu'].vertices[0,:,0].cpu().numpy().min(), item['outpu'].vertices[0,:,0].cpu().numpy().max(), \\\n",
    "    item['outpu'].vertices[0,:,1].cpu().numpy().min(), item['outpu'].vertices[0,:,1].cpu().numpy().max(), \\\n",
    "    item['outpu'].vertices[0,:,2].cpu().numpy().min(), item['outpu'].vertices[0,:,2].cpu().numpy().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
